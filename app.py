# app.py â€”â€” åªè¯» CSV çš„ Streamlit çœ‹æ¿ï¼ˆæ— å¤–éƒ¨ API è°ƒç”¨ï¼‰
import os
from datetime import date, timedelta

import altair as alt
import pandas as pd
import streamlit as st

st.set_page_config(page_title="YouTube Tracker", layout="wide")

# å¯é€‰ï¼šé¡µé¢è‡ªåŠ¨åˆ·æ–°ï¼ˆè‹¥æœªå®‰è£…åˆ™è‡ªåŠ¨è·³è¿‡ï¼‰
try:
    from streamlit_autorefresh import st_autorefresh
    st_autorefresh(interval=5 * 60 * 1000, key="auto-refresh")  # æ¯5åˆ†é’Ÿåˆ·æ–°ä¸€æ¬¡é¡µé¢
except Exception:
    pass

# ---- å°æ ·å¼ï¼šè®©å·¦ä¾§ç¼©ç•¥å›¾å‚ç›´å±…ä¸­ ----
st.markdown(
    """
<style>
.thumb-cell { display: flex; align-items: center; height: 100%; }
.thumb-cell img { max-width: 100%; }
</style>
""",
    unsafe_allow_html=True,
)

# ====== å·¥å…·å‡½æ•° ======
def coerce_date_range(picked, fallback_start, fallback_end):
    """æŠŠ st.date_input çš„è¿”å›å€¼ï¼ˆtuple/list/singleï¼‰è§„èŒƒåŒ–ä¸º (start_date, end_date)ã€‚"""
    if isinstance(picked, (list, tuple)):
        if len(picked) == 2:
            s, e = picked
        elif len(picked) == 1:
            s, e = picked[0], picked[0]
        else:
            s, e = fallback_start, fallback_end
    else:
        # å•æ—¥æ¨¡å¼
        s, e = picked, picked

    # å…œåº•ï¼šç©ºå€¼åˆ™å›é€€
    s = s or fallback_start
    e = e or fallback_end
    # è‹¥ç”¨æˆ·è¯¯é€‰â€œå¼€å§‹ > ç»“æŸâ€ï¼Œåˆ™å¯¹è°ƒ
    if s > e:
        s, e = e, s
    return s, e


def days_since(d):
    """è¿”å›ä»å‘å¸ƒæ—¶é—´åˆ°ç°åœ¨çš„å¤©æ•°ï¼›å…¼å®¹ tz-naive / tz-awareã€‚"""
    if pd.isna(d):
        return None
    if getattr(d, "tzinfo", None) is None:
        d_utc = d.tz_localize("UTC")
    else:
        d_utc = d.tz_convert("UTC")
    now_utc = pd.Timestamp.now(tz="UTC")
    return (now_utc - d_utc).days


# æ¯5åˆ†é’Ÿé‡æ–°è¯»ä¸€æ¬¡ CSVï¼ˆçº¿ä¸Šè‡ªåŠ¨æ‹¿åˆ°æœ€æ–°æ•°æ®ï¼‰
@st.cache_data(ttl=300)
def load_data():
    df = pd.read_csv("data/history.csv")
    # å…³é”®åˆ—ç»Ÿä¸€ä¸º tz-aware UTC
    df["date"] = pd.to_datetime(df["date"], errors="coerce", utc=True)
    df["published_at"] = pd.to_datetime(df["published_at"], errors="coerce", utc=True)
    return df


# ====== ä¸»ä½“ ======
df = load_data()

st.title("ğŸ“ˆ YouTube è§†é¢‘è¿½è¸ªé¢æ¿")

if df.empty:
    st.info("æš‚æ— æ•°æ®ï¼Œè¯·å…ˆç¡®ä¿ä»“åº“ä¸­çš„ data/history.csv å·²æœ‰å†…å®¹ã€‚")
    st.stop()

# ---- æ•°æ®æœ€åæ›´æ–°æ—¶é—´ï¼ˆåŸºäº CSV å†…å®¹ + æ–‡ä»¶å†™å…¥æ—¶é—´ï¼‰----
csv_last_ts = pd.to_datetime(df["date"], errors="coerce").max()
last_file_time_la = None
try:
    mtime = os.path.getmtime("data/history.csv")
    last_file_time_la = (
        pd.to_datetime(mtime, unit="s", utc=True)
        .tz_convert("America/Los_Angeles")
        .strftime("%Y-%m-%d %H:%M:%S %Z")
    )
except Exception:
    pass

msg_left = (
    f"CSV æœ€æ–°æ—¥æœŸï¼š**{csv_last_ts.tz_convert('UTC').date().isoformat()}**"
    if pd.notna(csv_last_ts)
    else "CSV æœ€æ–°æ—¥æœŸï¼š**æœªçŸ¥**"
)
msg_right = f"ï½œ æ–‡ä»¶æ›´æ–°æ—¶é—´ï¼ˆLAï¼‰ï¼š**{last_file_time_la}**" if last_file_time_la else ""
st.info(f"ğŸ•’ {msg_left} {msg_right}")

# ---- è®¡ç®—â€œæ¯ä¸ªè§†é¢‘æœ€æ–°ä¸€è¡Œâ€ä¸é»˜è®¤æ’åº ----
latest = df.sort_values("date").groupby("video_id", as_index=False).tail(1).copy()
latest = latest.sort_values("published_at", ascending=False, na_position="last")

# -------- ä¾§è¾¹ç­›é€‰ --------
with st.sidebar:
    st.header("ç­›é€‰ & å·¥å…·")

    # é¢‘é“ç­›é€‰ï¼ˆå« Allï¼‰
    channels = sorted(latest["channel_title"].dropna().unique().tolist())
    channel_options = ["All"] + channels
    sel_channel = st.selectbox("æŒ‰é¢‘é“ç­›é€‰", channel_options, index=0, key="channel_select")

    # æŒ‡æ ‡ä¸æ•°å€¼æ¨¡å¼
    metric_label = st.selectbox(
        "æŠ˜çº¿å›¾æŒ‡æ ‡", ["æ’­æ”¾é‡ (Views)", "ç‚¹èµæ•° (Likes)", "è¯„è®ºæ•° (Comments)"], index=0, key="metric_select"
    )
    metric_map = {
        "æ’­æ”¾é‡ (Views)": ("views", "æ’­æ”¾é‡"),
        "ç‚¹èµæ•° (Likes)": ("likes", "ç‚¹èµæ•°"),
        "è¯„è®ºæ•° (Comments)": ("comments", "è¯„è®ºæ•°"),
    }
    metric_col, metric_cn = metric_map[metric_label]

    mode = st.radio("æ•°å€¼æ¨¡å¼", ["ç´¯è®¡", "æ¯æ—¥å¢é‡"], index=0, horizontal=True, key="mode_radio")

    # æ—¥æœŸèŒƒå›´ï¼ˆå½±å“ï¼šæŠ˜çº¿å›¾ã€é¡¶éƒ¨åŒºé—´å¢é‡KPIï¼‰
    min_d = df["date"].min()
    max_d = df["date"].max()
    # è½¬ä¸ºæ—¥æœŸï¼ˆå»æ—¶åŒºï¼‰
    min_date = min_d.tz_convert("UTC").date() if pd.notna(min_d) else date.today()
    max_date = max_d.tz_convert("UTC").date() if pd.notna(max_d) else date.today()

    # å¢åŠ  key ä¸ on_change ç¡®ä¿å˜æ›´å³è§¦å‘é‡ç®—
    picked = st.date_input(
        "æŠ˜çº¿å›¾æ—¥æœŸèŒƒå›´",
        value=(min_date, max_date),
        key="date_range",
    )
    start_date, end_date = coerce_date_range(picked, min_date, max_date)

    # ä¾§æ å¯è§†åŒ–å›æ˜¾ï¼ˆä¾¿äºä½ æ ¸å¯¹å½“å‰å€¼ï¼‰
    st.caption(f"å·²é€‰æ—¥æœŸï¼š{start_date} â†’ {end_date}")

    # æ’åºä¾æ®ï¼ˆå«â€œæŒ‰å‘å¸ƒæ—¥æœŸï¼ˆæ–°â†’æ—§ï¼‰â€ï¼‰
    sort_label = st.selectbox(
        "æ’åºä¾æ®", ["æŒ‰æ’­æ”¾é‡", "æŒ‰ç‚¹èµæ•°", "æŒ‰è¯„è®ºæ•°", "æŒ‰å‘å¸ƒæ—¥æœŸï¼ˆæ–°â†’æ—§ï¼‰"], index=3, key="sort_select"
    )
    sort_map = {"æŒ‰æ’­æ”¾é‡": "views", "æŒ‰ç‚¹èµæ•°": "likes", "æŒ‰è¯„è®ºæ•°": "comments"}

    st.write("---")
    # æ‰‹åŠ¨åˆ·æ–°æŒ‰é’®ï¼ˆæ¸…ç¼“å­˜å¹¶é‡è·‘ï¼‰
    if st.button("ğŸ”„ åˆ·æ–°æ•°æ®ï¼ˆæ¸…ç¼“å­˜ï¼‰", key="refresh"):
        st.cache_data.clear()
        st.rerun()

# æ ¹æ®é¢‘é“ç­›é€‰
filtered_latest = latest if sel_channel == "All" else latest[latest["channel_title"] == sel_channel]

# åº”ç”¨æ’åº
if sort_label == "æŒ‰å‘å¸ƒæ—¥æœŸï¼ˆæ–°â†’æ—§ï¼‰":
    filtered_latest = filtered_latest.sort_values("published_at", ascending=False, na_position="last")
else:
    sort_col = sort_map[sort_label]
    filtered_latest = filtered_latest.sort_values(sort_col, ascending=False)

selected_ids = set(filtered_latest["video_id"].tolist())

# ---- å°†æ—¥æœŸè¾¹ç•Œè§„èŒƒä¸º UTC çš„é—­åŒºé—´ [start, end] ----
start_ts_utc = pd.to_datetime(start_date).tz_localize("UTC")
end_ts_utc = (pd.to_datetime(end_date) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)).tz_localize("UTC")

# ---- å†å²ç­›é€‰ï¼ˆä¾›å›¾è¡¨/å¯¹æ¯”ä½¿ç”¨ï¼‰ ----
hist_df = df[df["video_id"].isin(selected_ids)].copy()
show_df_for_chart = hist_df[(hist_df["date"] >= start_ts_utc) & (hist_df["date"] <= end_ts_utc)].copy()

# å¦‚æœé€‰æ‹©çš„ç»“æŸæ—¥æœŸ > æ•°æ®æœ€æ–°æ—¥æœŸï¼Œæç¤º
data_max_day = max_date
if data_max_day and end_date > data_max_day:
    st.warning(f"æ‰€é€‰ç»“æŸæ—¥æœŸ **{end_date}** è¶…è¿‡å½“å‰æ•°æ®æœ€æ–°æ—¥æœŸ **{data_max_day}**ï¼Œå›¾è¡¨åªæ˜¾ç¤ºåˆ° {data_max_day}ã€‚")

st.caption(f"æ•°æ®æŒ‰å¤©è®°å½•ï¼›é¢‘é“ï¼š{sel_channel} ï½œ è§†é¢‘æ•°ï¼š{filtered_latest.shape[0]}")

# ---- å…¨å±€ KPIï¼ˆæ€»é‡/ç‡ï¼‰ï¼šé’ˆå¯¹å½“å‰é¢‘é“ç­›é€‰ï¼ˆå„è§†é¢‘â€œæœ€æ–°ä¸€è¡Œâ€åŠ æ€»ï¼‰ ----
kpi_scope = filtered_latest.copy()
total_views = int(kpi_scope["views"].sum())
total_likes = int(kpi_scope["likes"].sum())
total_comments = int(kpi_scope["comments"].sum())
like_rate = (total_likes / total_views * 100) if total_views > 0 else 0.0
comment_rate = (total_comments / total_views * 100) if total_views > 0 else 0.0

k1, k2, k3, k4, k5 = st.columns(5)
k1.metric("æ€»æ’­æ”¾é‡ï¼ˆæˆªè‡³æœ€æ–°ï¼‰", f"{total_views:,}")
k2.metric("æ€»ç‚¹èµæ•°ï¼ˆæˆªè‡³æœ€æ–°ï¼‰", f"{total_likes:,}")
k3.metric("æ€»è¯„è®ºæ•°ï¼ˆæˆªè‡³æœ€æ–°ï¼‰", f"{total_comments:,}")
k4.metric("Like Rateï¼ˆç‚¹èµç‡ï¼‰", f"{like_rate:.2f}%")
k5.metric("Comment Rateï¼ˆè¯„è®ºç‡ï¼‰", f"{comment_rate:.2f}%")

# ===== é¡¶éƒ¨ KPI æ±‡æ€»ï¼ˆä¸¥æ ¼æŒ‰æ‰€é€‰æ—¥æœŸèŒƒå›´çš„â€œåŒºé—´å¢é‡â€ï¼‰ =====
# å…ˆåœ¨å…¨é‡å†å²ä¸Šè®¡ç®—â€œæ¯æ—¥å¢é‡â€ï¼Œå†æŒ‰æ‰€é€‰æ—¥æœŸèŒƒå›´åˆ‡ç‰‡æ±‚å’Œ
base_df = df[df["video_id"].isin(selected_ids)].sort_values(["video_id", "date"]).copy()
for col in ["views", "likes", "comments"]:
    inc_col = f"{col}_inc"
    base_df[inc_col] = base_df.groupby("video_id")[col].diff().fillna(0)
    base_df.loc[base_df[inc_col] < 0, inc_col] = 0  # é˜²æŠ–ï¼šå‡ºç°å›é€€æ—¶ä¸è®¡è´Ÿå¢é‡

interval_df = base_df[(base_df["date"] >= start_ts_utc) & (base_df["date"] <= end_ts_utc)].copy()

iv_views = int(interval_df["views_inc"].sum()) if not interval_df.empty else 0
iv_likes = int(interval_df["likes_inc"].sum()) if not interval_df.empty else 0
iv_comments = int(interval_df["comments_inc"].sum()) if not interval_df.empty else 0

i1, i2, i3 = st.columns(3)
i1.metric("æœ¬æœŸæ€»å¢é‡ Â· æ’­æ”¾é‡", f"{iv_views:,}")
i2.metric("æœ¬æœŸæ€»å¢é‡ Â· ç‚¹èµæ•°", f"{iv_likes:,}")
i3.metric("æœ¬æœŸæ€»å¢é‡ Â· è¯„è®ºæ•°", f"{iv_comments:,}")

# ====== å„è§†é¢‘å•å¡ç‰‡ + æŠ˜çº¿ï¼ˆå¸¦ç‚¹ä¸æ•°å€¼æ ‡ç­¾ï¼‰ ======
for _, row in filtered_latest.iterrows():
    vid = row["video_id"]
    col1, col2 = st.columns([1, 3])

    with col1:
        thumb = row.get("thumbnail_url", None)
        st.markdown("<div class='thumb-cell'>", unsafe_allow_html=True)
        if pd.notna(thumb) and thumb:
            st.image(thumb, use_container_width=True)
        st.markdown("</div>", unsafe_allow_html=True)
        st.markdown(f"[â–¶ï¸ æ‰“å¼€è§†é¢‘]({row['video_url']})")

    with col2:
        st.subheader(f"{row['title']}")
        st.write(f"**é¢‘é“**ï¼š{row['channel_title']}")
        pub = row["published_at"]
        dcount = days_since(pub)
        pub_text = pub.tz_convert("UTC").date().isoformat() if pd.notna(pub) else "æœªçŸ¥"
        st.write(f"**å‘å¸ƒæ—¥æœŸ**ï¼š{pub_text}  ï½œ  **å·²å‘å¸ƒ**ï¼š{dcount} å¤©")
        c1, c2, c3 = st.columns(3)
        c1.metric("æ€»æ’­æ”¾é‡", f"{int(row['views']):,}")
        c2.metric("æ€»ç‚¹èµæ•°", f"{int(row['likes']):,}")
        c3.metric("æ€»è¯„è®ºæ•°", f"{int(row['comments']):,}")

        vhist = show_df_for_chart[show_df_for_chart["video_id"] == vid].sort_values("date").copy()
        if vhist.empty:
            st.info("å½“å‰æ—¥æœŸèŒƒå›´å†…æ— æ•°æ®")
            continue

        if mode == "æ¯æ—¥å¢é‡":
            vhist["value"] = vhist[metric_col].diff().fillna(0)
            vhist.loc[vhist["value"] < 0, "value"] = 0
            y_title = f"{metric_cn}ï¼ˆæ¯æ—¥å¢é‡ï¼‰"
        else:
            vhist["value"] = vhist[metric_col]
            y_title = f"{metric_cn}ï¼ˆç´¯è®¡ï¼‰"

        chart_base = alt.Chart(vhist).encode(
            x=alt.X("date:T", title="æ—¥æœŸ"),
            y=alt.Y("value:Q", title=y_title),
            tooltip=[
                alt.Tooltip("date:T", title="æ—¥æœŸ"),
                alt.Tooltip("value:Q", title=y_title, format=","),
            ],
        )
        line = chart_base.mark_line()
        points = chart_base.mark_point(size=40)
        labels = chart_base.mark_text(dy=-8).encode(text=alt.Text("value:Q", format=","))

        chart = (line + points + labels).properties(height=220)
        st.altair_chart(chart, use_container_width=True)

# ====== å¤šè§†é¢‘å¯¹æ¯”ï¼ˆä¸€å¼ å›¾ï¼‰ ======
st.write("---")
st.subheader("ğŸ“Š å¤šè§†é¢‘å¯¹æ¯”ï¼ˆåŒä¸€å¼ å›¾ï¼‰")

label_map = dict(
    zip(
        filtered_latest["video_id"],
        filtered_latest["title"] + " â€” " + filtered_latest["channel_title"],
    )
)
default_compare_ids = list(label_map.keys())
compare_labels = st.multiselect(
    "é€‰æ‹©å‚ä¸å¯¹æ¯”çš„è§†é¢‘",
    options=[label_map[v] for v in default_compare_ids],
    default=[label_map[v] for v in default_compare_ids],
    key="compare_select",
)
compare_ids = {vid for vid, label in label_map.items() if label in compare_labels}

cmp = show_df_for_chart[show_df_for_chart["video_id"].isin(compare_ids)].copy()
if cmp.empty:
    st.info("å½“å‰ç­›é€‰ä¸‹æ²¡æœ‰å¯å¯¹æ¯”çš„æ•°æ®")
else:
    cmp = cmp.sort_values(["video_id", "date"]).copy()
    if mode == "æ¯æ—¥å¢é‡":
        cmp["value"] = cmp.groupby("video_id")[metric_col].diff().fillna(0)
        cmp.loc[cmp["value"] < 0, "value"] = 0
        y_title = f"{metric_cn}ï¼ˆæ¯æ—¥å¢é‡ï¼‰"
    else:
        cmp["value"] = cmp[metric_col]
        y_title = f"{metric_cn}ï¼ˆç´¯è®¡ï¼‰"

    cmp["label"] = cmp["video_id"].map(label_map)
    legend_sel = alt.selection_point(fields=["label"], bind="legend", toggle=True)

    base_cmp = (
        alt.Chart(cmp)
        .transform_filter(legend_sel)
        .encode(
            x=alt.X("date:T", title="æ—¥æœŸ"),
            y=alt.Y("value:Q", title=y_title),
            color=alt.Color("label:N", title="è§†é¢‘"),
            tooltip=[
                alt.Tooltip("label:N", title="è§†é¢‘"),
                alt.Tooltip("date:T", title="æ—¥æœŸ"),
                alt.Tooltip("value:Q", title=y_title, format=","),
            ],
        )
    ).add_params(legend_sel)

    line_cmp = base_cmp.mark_line()
    points_cmp = base_cmp.mark_point(size=36)
    labels_cmp = base_cmp.mark_text(dy=-8).encode(text=alt.Text("value:Q", format=","))

    compare_chart = (line_cmp + points_cmp + labels_cmp).properties(height=360)
    st.altair_chart(compare_chart, use_container_width=True)

    # === å¯¹æ¯”è¡¨æ ¼ï¼ˆç›´è§‚æ±‡æ€»ï¼Œä»…ä¿ç•™ä¸‹è½½è¡¨æ ¼ CSVï¼‰ ===
    meta_cols = ["channel_title", "title", "published_at", "video_url"]
    meta_map = filtered_latest.set_index("video_id")[meta_cols].to_dict(orient="index")

    rows = []
    for vid, g in cmp.groupby("video_id"):
        g = g.sort_values("date")
        first_dt = g["date"].min()
        last_dt = g["date"].max()
        points_cnt = g.shape[0]
        if mode == "æ¯æ—¥å¢é‡":
            metric_val = g["value"].sum()                # åŒºé—´æ€»å¢é‡
            peak_val = g["value"].max()                  # æœ€å¤§å•æ—¥å¢é‡
            metric_label_cn = f"{metric_cn} Â· åŒºé—´æ€»å¢é‡"
            peak_label_cn = f"æœ€å¤§å•æ—¥å¢é‡"
        else:
            metric_val = g["value"].iloc[-1]             # åŒºé—´æœ«å€¼ï¼ˆç´¯è®¡ï¼‰
            peak_val = g["value"].max()                  # ç´¯è®¡æœ€å¤§å€¼ï¼ˆä¸€èˆ¬=æœ«å€¼ï¼‰
            metric_label_cn = f"{metric_cn} Â· åŒºé—´æœ«å€¼"
            peak_label_cn = f"åŒºé—´æœ€å¤§å€¼"

        avg_val = g["value"].mean() if points_cnt > 0 else 0

        meta = meta_map.get(vid, {})
        pub = meta.get("published_at")
        pub_text = (
            pd.to_datetime(pub, utc=True).tz_convert("UTC").date().isoformat()
            if pd.notna(pub) else "â€”"
        )

        rows.append({
            "è§†é¢‘æ ‡é¢˜": meta.get("title", "â€”"),
            "é¢‘é“": meta.get("channel_title", "â€”"),
            "è§†é¢‘ID": vid,
            "å‘å¸ƒæ—¥æœŸ": pub_text,
            "åŒºé—´å¼€å§‹": first_dt.tz_convert("UTC").date().isoformat(),
            "åŒºé—´ç»“æŸ": last_dt.tz_convert("UTC").date().isoformat(),
            metric_label_cn: int(metric_val) if pd.notna(metric_val) else 0,
            "æ—¥å‡å€¼": round(avg_val, 2) if pd.notna(avg_val) else 0,
            peak_label_cn: int(peak_val) if pd.notna(peak_val) else 0,
            "é“¾æ¥": meta.get("video_url", "â€”"),
        })

    summary_df = pd.DataFrame(rows)

    st.markdown("#### ğŸ“‹ å¯¹æ¯”è¡¨æ ¼ï¼ˆå½“å‰æŒ‡æ ‡ & æ¨¡å¼ä¸‹çš„åŒºé—´è¡¨ç°ï¼‰")
    st.dataframe(
        summary_df[
            ["è§†é¢‘æ ‡é¢˜", "é¢‘é“", "è§†é¢‘ID", "å‘å¸ƒæ—¥æœŸ", "åŒºé—´å¼€å§‹", "åŒºé—´ç»“æŸ",
             metric_label_cn, "æ—¥å‡å€¼", peak_label_cn, "é“¾æ¥"]
        ],
        use_container_width=True,
        hide_index=True
    )

    # ä»…ä¿ç•™ï¼šä¸‹è½½è¡¨æ ¼ CSV
    table_csv = summary_df.to_csv(index=False).encode("utf-8")
    st.download_button(
        label="â¬‡ï¸ ä¸‹è½½å¯¹æ¯”è¡¨æ ¼ï¼ˆCSVï¼‰",
        data=table_csv,
        file_name="compare_table.csv",
        mime="text/csv",
    )

st.write("---")
st.caption("æ•°æ®æ¥æºï¼šdata/history.csvï¼ˆç”±å®šæ—¶ä»»åŠ¡æ›´æ–°ï¼‰ã€‚æ—¶åŒºï¼šAmerica/Los_Angelesã€‚")
